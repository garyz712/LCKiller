{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtqfza5i-1d-",
        "outputId": "ec40d9b3-3eb8-4d07-bf31-5ea78c63730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 2.3031, Accuracy: 0.1070\n",
            "Epoch [2/5], Loss: 2.2997, Accuracy: 0.1500\n",
            "Epoch [3/5], Loss: 2.2961, Accuracy: 0.1610\n",
            "Epoch [4/5], Loss: 2.2916, Accuracy: 0.1530\n",
            "Epoch [5/5], Loss: 2.2834, Accuracy: 0.1610\n",
            "Test Accuracy: 0.1150\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class smallMLP(nn.Module):\n",
        "  def __init__(self, input_size, output_size=10):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.softmax(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "# Step 1: Define a toy dataset\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, input_size=20, num_classes=10):\n",
        "        self.num_samples = num_samples\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Generate random input data and corresponding labels\n",
        "        self.data = torch.randn(num_samples, input_size)\n",
        "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# Training Loop -> IMPORTANT !\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epoch=5, device='cpu'):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_epoch):\n",
        "    total_loss, total, correct = 0, 0, 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "      outputs = model(batch_x)\n",
        "      loss = criterion(outputs, batch_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, dim=-1)\n",
        "      correct += (predicted==batch_y).sum().item()\n",
        "      total += batch_y.size(0)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, device='cpu'):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  correct, total = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_x, batch_y in val_loader:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "      outputs = model(batch_x)\n",
        "      _, predicted = torch.max(outputs, dim=-1)\n",
        "      correct += (predicted==batch_y).sum().item()\n",
        "      total += batch_y.size(0)\n",
        "\n",
        "  print(f\"Test Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    input_size = 20\n",
        "    num_classes = 10\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Create toy datasets\n",
        "    train_dataset = ToyDataset(num_samples=1000, input_size=input_size, num_classes=num_classes)\n",
        "    test_dataset = ToyDataset(num_samples=200, input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = smallMLP(input_size, num_classes)\n",
        "    criterion = nn.CrossEntropyLoss() #important\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #important\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "    evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/junyang/miniconda3/lib/python3.13/site-packages (2.3.5)\n",
            "Requirement already satisfied: torch in /Users/junyang/miniconda3/lib/python3.13/site-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in /Users/junyang/miniconda3/lib/python3.13/site-packages (0.24.1)\n",
            "Requirement already satisfied: pillow in /Users/junyang/miniconda3/lib/python3.13/site-packages (12.0.0)\n",
            "Requirement already satisfied: filelock in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/junyang/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 – Install/upgrade the required packages (run once)\n",
        "!pip install --upgrade numpy torch torchvision pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transfer Learning\n",
        "\n",
        "\"\"\"\n",
        "In relu.py, we implemented a simple neural network with a single hidden layer using the ReLU activation function.\n",
        "\n",
        "Now, let's move on to the next task. The goal is to extract features from a pretrained AlexNet model and use those features to train a two-layer fully connected network on the CIFAR-10 dataset.\n",
        "\n",
        "Specifically, you need to:\n",
        "1. Extract features from the pretrained AlexNet model up to the fc2 layer (before the final classification layer).\n",
        "2. Build a two-layer fully connected network that takes the extracted features as input and classifies the CIFAR-10 dataset (10 classes). Use the ReLU activation function in the hidden layer.\n",
        "3. Define a custom loss function that combines standard cross-entropy loss with an L1 regularization term on the model parameters.\n",
        "4. Train the two-layer fully connected network on the CIFAR-10 dataset using the custom loss function.\n",
        "\n",
        "Here's some useful code hint to consider:\n",
        "class SampleNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SampleNetwork, self).__init__()\n",
        "        # TODO: Initialization code goes here\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Forward pass code goes here\n",
        "        pass\n",
        "\"\"\"\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "def load_cifar10_dataset() -> CIFAR10:\n",
        "    \"\"\"\n",
        "    Load CIFAR-10 dataset and apply transformations accordingly\n",
        "\n",
        "    Returns:\n",
        "        train_dataset (torchvision.datasets.cifar.CIFAR10): CIFAR-10 train dataset\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "    train_dataset = CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "    return train_dataset\n",
        "\n",
        "class SampleNetwork(nn.Module):\n",
        "    def __init__(self, alexnet):\n",
        "        super(SampleNetwork, self).__init__()    \n",
        "        self.alexnet = alexnet\n",
        "        self.fc1 = nn.Linear(4096, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Forward pass code goes here\n",
        "        out1 = self.alexnet.features(input)\n",
        "        out2 = self.alexnet.avgpool(out1)\n",
        "        out2 = torch.flatten(out2, 1)\n",
        "        out3 = self.alexnet.classifier[:-1](out2)\n",
        "\n",
        "        out4 = self.fc1(out3)\n",
        "        out5 = self.relu(out4)\n",
        "        return self.fc2(out5)\n",
        "\n",
        "def main(args: argparse.Namespace) -> None:\n",
        "    \"\"\"\n",
        "    Main function of the script\n",
        "\n",
        "    Args:\n",
        "        args (argparse.Namespace): Hyperparameters\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load pretrained AlexNet\n",
        "    alexnet = models.alexnet(pretrained=True).to(device)\n",
        "    # Freeze AlexNet parameters\n",
        "    for param in alexnet.parameters():\n",
        "        param.requires_grad = False\n",
        "    alexnet.eval()\n",
        "\n",
        "    print(alexnet)\n",
        "    \n",
        "\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    train_dataset = load_cifar10_dataset()\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # TODO Put your code here\n",
        "    MyModel = SampleNetwork(alexnet).to(device)\n",
        "\n",
        "    # Set optimizer - only optimize parameters that require gradients\n",
        "    parameters = [p for p in MyModel.parameters() if p.requires_grad]\n",
        "    \n",
        "    assert parameters is not None, \"Define parameters for optimizer\"\n",
        "    optimizer = torch.optim.Adam(\n",
        "        parameters,\n",
        "        lr=args.learning_rate\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for _ in range(args.n_epoch):\n",
        "        for inputs, labels in train_loader:\n",
        "            # TODO Your code goes here\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            output = MyModel(inputs)\n",
        "            print(output.shape, )\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()  # TODO Put your code here\n",
        "            loss = loss_fn(output, labels)           \n",
        "\n",
        "            \n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "main(argparse.Namespace(learning_rate=0.001, batch_size=32, n_epoch=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Fold Cross Validation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def k_fold_cv(X, y, k=5, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    idx = np.random.permutation(len(X))\n",
        "    fold_size = len(X) // k\n",
        "    scores = []\n",
        "\n",
        "    for i in range(k):\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size if i < k - 1 else len(X)\n",
        "        test_idx = idx[start:end]\n",
        "        train_idx = np.concatenate([idx[:start], idx[end:]])\n",
        "\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Reinitialize your model here\n",
        "        model = lambda x: np.dot(x, np.linalg.pinv(X_train) @ y_train)  # Example\n",
        "\n",
        "        y_pred = model(X_test)\n",
        "        acc = np.mean((y_pred > 0.5) == y_test)\n",
        "        scores.append(acc)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Usage\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.random.randint(0, 2, 100)\n",
        "print(\"Accuracy:\", k_fold_cv(X, y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2tkJZrUPnBLk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, SSE: 0.0200\n",
            "Iteration 2, SSE: 0.0100\n",
            "Converged early.\n",
            "Final centroids: [[10.0, 10.0], [1.05, 1.05]]\n",
            "Prediction for [1.2, 1.2]: [1]\n"
          ]
        }
      ],
      "source": [
        "# K-Means (fixed with minimal changes)\n",
        "import math\n",
        "import random\n",
        "\n",
        "# Datapoints (x1, x2, y) — we only use x1,x2 for clustering\n",
        "class Kmeans:\n",
        "    '''Implementing Kmeans algorithm.'''\n",
        "\n",
        "    def __init__(self, n_clusters, max_iter=100, random_state=123):\n",
        "        self.number_clusters = n_clusters \n",
        "        self.iteration = max_iter\n",
        "        self.centroids = []\n",
        "        if random_state:\n",
        "            random.seed(random_state)\n",
        "        \n",
        "    def initialize_centroids(self, X):\n",
        "        # FIX 1: Actually pick real data points (kmeans++ style is better, but at least not random [0-1])\n",
        "        # Simple fix: randomly select k points from X\n",
        "        self.centroids = random.sample(X, self.number_clusters)\n",
        "        # Extract only the coordinates (x1, x2), ignore y\n",
        "        self.centroids = [[x[0], x[1]] for x in self.centroids]\n",
        "        return self.centroids\n",
        "        \n",
        "    def compute_centroids(self, X, labels):\n",
        "        number_points_per_cluster = [0] * self.number_clusters\n",
        "        # FIX 2: Don't do [[0,0]] * k — this creates references to same list!\n",
        "        self.centroids = [[0.0, 0.0] for _ in range(self.number_clusters)]\n",
        "        \n",
        "        for i in range(len(X)):\n",
        "            x1, x2, y = X[i]\n",
        "            label = labels[i]\n",
        "            number_points_per_cluster[label] += 1\n",
        "            self.centroids[label][0] += x1\n",
        "            self.centroids[label][1] += x2\n",
        "            \n",
        "        for i in range(self.number_clusters):\n",
        "            if number_points_per_cluster[i] > 0:  # avoid division by zero\n",
        "                self.centroids[i][0] /= number_points_per_cluster[i]\n",
        "                self.centroids[i][1] /= number_points_per_cluster[i]\n",
        "            # else: keep previous centroid (or could reinitialize)\n",
        "            \n",
        "        return self.centroids\n",
        "\n",
        "    def compute_distance(self, X, centroids):\n",
        "        res = []\n",
        "        for i in range(len(X)):\n",
        "            x1, x2, y = X[i]\n",
        "            distances = []\n",
        "            for c1, c2 in centroids:           \n",
        "                distance = math.sqrt((x1 - c1)**2 + (x2 - c2)**2)\n",
        "                distances.append(distance)\n",
        "            res.append(distances)\n",
        "        return res\n",
        "\n",
        "    def find_closest_cluster(self, distance):\n",
        "        ans = []\n",
        "        for i in range(len(distance)):\n",
        "            res = 0\n",
        "            smallest = float(\"inf\")\n",
        "            for j in range(len(distance[i])):\n",
        "                if distance[i][j] < smallest:\n",
        "                    smallest = distance[i][j]\n",
        "                    res = j\n",
        "            ans.append(res)\n",
        "        return ans\n",
        "\n",
        "    def compute_sse(self, X, labels):\n",
        "        res = 0\n",
        "        for i in range(len(X)):\n",
        "            x1, x2, y = X[i]\n",
        "            label = labels[i]\n",
        "            c1, c2 = self.centroids[label]\n",
        "            res += (x1 - c1)**2 + (x2 - c2)**2\n",
        "        return res\n",
        "            \n",
        "    def fit(self, X):\n",
        "        self.initialize_centroids(X)\n",
        "        for i in range(self.iteration):\n",
        "            labels = self.predict(X)\n",
        "            error = self.compute_sse(X, labels)\n",
        "            print(f\"Iteration {i+1}, SSE: {error:.4f}\")\n",
        "            \n",
        "            old_centroids = [c[:] for c in self.centroids]\n",
        "            self.compute_centroids(X, labels)\n",
        "            \n",
        "            # Optional: early stopping if centroids don't move\n",
        "            if old_centroids == self.centroids:\n",
        "                print(\"Converged early.\")\n",
        "                break\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # X can be list of [x1,x2,y] or just [x1,x2]\n",
        "        distances = self.compute_distance(X, self.centroids)\n",
        "        return self.find_closest_cluster(distances)\n",
        "\n",
        "\n",
        "# Test\n",
        "X = [[1, 1, 100], [1.1, 1.1, 200], [10, 10, 10000]]\n",
        "k = 2\n",
        "kmean = Kmeans(k, random_state=42)\n",
        "kmean.fit(X)\n",
        "print(\"Final centroids:\", kmean.centroids)\n",
        "print(\"Prediction for [1.2, 1.2]:\", kmean.predict([[1.2, 1.2, 100]]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
